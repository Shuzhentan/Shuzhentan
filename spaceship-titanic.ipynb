{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-22T13:55:29.500241Z","iopub.execute_input":"2022-08-22T13:55:29.500797Z","iopub.status.idle":"2022-08-22T13:55:29.541548Z","shell.execute_reply.started":"2022-08-22T13:55:29.500677Z","shell.execute_reply":"2022-08-22T13:55:29.540551Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_set = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\ntest_set = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:29.910226Z","iopub.execute_input":"2022-08-22T13:55:29.911107Z","iopub.status.idle":"2022-08-22T13:55:29.999889Z","shell.execute_reply.started":"2022-08-22T13:55:29.911053Z","shell.execute_reply":"2022-08-22T13:55:29.998262Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_set.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:30.192892Z","iopub.execute_input":"2022-08-22T13:55:30.193351Z","iopub.status.idle":"2022-08-22T13:55:30.235447Z","shell.execute_reply.started":"2022-08-22T13:55:30.193314Z","shell.execute_reply":"2022-08-22T13:55:30.234224Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"All features have missing values, except `PassengerId`.","metadata":{}},{"cell_type":"code","source":"test_set.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:31.579376Z","iopub.execute_input":"2022-08-22T13:55:31.580264Z","iopub.status.idle":"2022-08-22T13:55:31.597989Z","shell.execute_reply.started":"2022-08-22T13:55:31.580217Z","shell.execute_reply":"2022-08-22T13:55:31.596035Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_set.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:31.821880Z","iopub.execute_input":"2022-08-22T13:55:31.822595Z","iopub.status.idle":"2022-08-22T13:55:31.854188Z","shell.execute_reply.started":"2022-08-22T13:55:31.822558Z","shell.execute_reply":"2022-08-22T13:55:31.852981Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Before diving more into the dataset, I will \n\n- create `Deck` and `Side` from `Cabin`\n- create `GroupSize` from `PassengerId`\n- remove `Name`, `PassengerId`, `Cabin`","metadata":{}},{"cell_type":"code","source":"# create 'Deck' and 'Side' from 'Cabin'\ndeck, side = [], []\n\nbool = train_set[\"Cabin\"].isnull()\nfor i in range(len(train_set)):\n    cabin = train_set[\"Cabin\"][i]\n    if bool[i]:\n        deck.append(np.NaN)\n        side.append(np.NaN)\n    else:\n        comp = cabin.split('/')\n        deck.append(comp[0])\n        side.append(comp[2])\n        \ntrain_set[\"Deck\"] = deck\ntrain_set[\"Side\"] = side","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:32.206869Z","iopub.execute_input":"2022-08-22T13:55:32.207352Z","iopub.status.idle":"2022-08-22T13:55:32.315706Z","shell.execute_reply.started":"2022-08-22T13:55:32.207314Z","shell.execute_reply":"2022-08-22T13:55:32.314462Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# create 'GroupSize' from 'PassengerId'\ntrain_set['GroupSize'] = [pid.split('_')[1] for pid in train_set.PassengerId]","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:32.417437Z","iopub.execute_input":"2022-08-22T13:55:32.417911Z","iopub.status.idle":"2022-08-22T13:55:32.431404Z","shell.execute_reply.started":"2022-08-22T13:55:32.417876Z","shell.execute_reply":"2022-08-22T13:55:32.429541Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Drop 'Name', 'PassengerId', 'Cabin'\ntrain_set = train_set.drop(['Name', 'PassengerId', 'Cabin'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:32.630378Z","iopub.execute_input":"2022-08-22T13:55:32.630824Z","iopub.status.idle":"2022-08-22T13:55:32.645417Z","shell.execute_reply.started":"2022-08-22T13:55:32.630775Z","shell.execute_reply":"2022-08-22T13:55:32.644187Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Brief Look at train and test sets**","metadata":{}},{"cell_type":"code","source":"train_set.info()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:33.099393Z","iopub.execute_input":"2022-08-22T13:55:33.099881Z","iopub.status.idle":"2022-08-22T13:55:33.119614Z","shell.execute_reply.started":"2022-08-22T13:55:33.099840Z","shell.execute_reply":"2022-08-22T13:55:33.117822Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- Categorical variables:\n\n     `HomePlanet`,`CryoSleep`, `Destination`, `VIP`,  `Deck`, `Side`, `GroupSize`\n\n- Numerical variables:\n\n     `Age`, `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck`","metadata":{}},{"cell_type":"code","source":"cat_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side', 'GroupSize']\nnum_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:33.599205Z","iopub.execute_input":"2022-08-22T13:55:33.599646Z","iopub.status.idle":"2022-08-22T13:55:33.613046Z","shell.execute_reply.started":"2022-08-22T13:55:33.599613Z","shell.execute_reply":"2022-08-22T13:55:33.611716Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_set.describe()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:34.321031Z","iopub.execute_input":"2022-08-22T13:55:34.321508Z","iopub.status.idle":"2022-08-22T13:55:34.365020Z","shell.execute_reply.started":"2022-08-22T13:55:34.321471Z","shell.execute_reply":"2022-08-22T13:55:34.363859Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization and Exploration**\n\n1. Histogram for labels \n\n2. Histogram for each categorical variable\n\n3. Scatterplot for each numerical variable\n\n4. Correlation for each numerical variable","metadata":{}},{"cell_type":"code","source":"# Histogram for labels\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nTransported_int = np.zeros([len(train_set)])\nTransported_int[train_set['Transported'] == True] = 1\nsns.displot(Transported_int)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:35.157132Z","iopub.execute_input":"2022-08-22T13:55:35.157859Z","iopub.status.idle":"2022-08-22T13:55:36.240658Z","shell.execute_reply.started":"2022-08-22T13:55:35.157819Z","shell.execute_reply":"2022-08-22T13:55:36.239027Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nSame size for each label.","metadata":{}},{"cell_type":"code","source":"# Histogram for each categorical variable\nfig, axes = plt.subplots(4, 2, figsize=(40,20))\nax = [axes[0,0], axes[0,1], axes[1,0], axes[1,1], axes[2,0], axes[2,1], axes[3,0]]\n\nfor i in range(len(cat_features)):\n    feature = cat_features[i]\n    sns.countplot(x=feature, hue='Transported', data=train_set, ax=ax[i])","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:36.242784Z","iopub.execute_input":"2022-08-22T13:55:36.243189Z","iopub.status.idle":"2022-08-22T13:55:37.781073Z","shell.execute_reply.started":"2022-08-22T13:55:36.243155Z","shell.execute_reply":"2022-08-22T13:55:37.777102Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for feature in cat_features:\n    s = train_set.groupby([feature])['Transported'].value_counts(normalize=True)\n    print(s)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:37.783921Z","iopub.execute_input":"2022-08-22T13:55:37.784325Z","iopub.status.idle":"2022-08-22T13:55:37.816305Z","shell.execute_reply.started":"2022-08-22T13:55:37.784293Z","shell.execute_reply":"2022-08-22T13:55:37.815095Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Scatterplot for each numerical variable\n\nfig, axes = plt.subplots(3, 2, figsize=(20,10))\nax = [axes[0,0], axes[0,1], axes[1,0], axes[1,1], axes[2,0], axes[2,1]]\n\nfor i in range(len(num_features)):\n    feature = num_features[i]\n    sns.set(style=\"darkgrid\")\n    sns.histplot(data=train_set, x=feature, hue=\"Transported\", bins=50, multiple='stack',\n                ax=ax[i])","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:37.818077Z","iopub.execute_input":"2022-08-22T13:55:37.818570Z","iopub.status.idle":"2022-08-22T13:55:40.398977Z","shell.execute_reply.started":"2022-08-22T13:55:37.818526Z","shell.execute_reply":"2022-08-22T13:55:40.397251Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# correlation for numerical variables\ncorr_matrix = train_set.corr()\ncorr_matrix","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:40.402098Z","iopub.execute_input":"2022-08-22T13:55:40.403510Z","iopub.status.idle":"2022-08-22T13:55:40.423351Z","shell.execute_reply.started":"2022-08-22T13:55:40.403462Z","shell.execute_reply":"2022-08-22T13:55:40.421821Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"corr_matrix[\"Transported\"]","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:40.425323Z","iopub.execute_input":"2022-08-22T13:55:40.425819Z","iopub.status.idle":"2022-08-22T13:55:40.436085Z","shell.execute_reply.started":"2022-08-22T13:55:40.425771Z","shell.execute_reply":"2022-08-22T13:55:40.435066Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- `RoomService`, `Spa`, `VRDeck` are correlated with `Transported`.\n-  Notice the correlation between `VRDeck`, `FoodCourt`, `Spa`.","metadata":{}},{"cell_type":"markdown","source":"# **Split the train_set into a smaller train_set and a test set**\n\nSplit the set into 2 groups such that both have same distributions of 'Transported'.","metadata":{}},{"cell_type":"code","source":"# Split the train_set into a smaller train_set and a test set\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, val_index in split.split(train_set, train_set['Transported']):\n    strain_set = train_set.loc[train_index]\n    val_set = train_set.loc[val_index]\n\nstrain_y = strain_set['Transported']\nstrain_x = strain_set.drop('Transported', axis=1)\n\nval_y = val_set['Transported']\nval_x = val_set.drop('Transported', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T13:55:40.437337Z","iopub.execute_input":"2022-08-22T13:55:40.437780Z","iopub.status.idle":"2022-08-22T13:55:40.610026Z","shell.execute_reply.started":"2022-08-22T13:55:40.437729Z","shell.execute_reply":"2022-08-22T13:55:40.608704Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# **Pipeline: Data cleaning + Addition of new features**","metadata":{}},{"cell_type":"markdown","source":"Implement the following tasks:\n\n1.  categorical features: \n        \n       - group `GroupSize` into 5 groups \n       \n       - group `Age` into 6 groups\n       \n       - replace missing values with respective most frequent categories\n        \n       - apply one-hot encoding for categorical features\n2.  numerical features:\n\n       - replace missing values with respective medians\n      \n       - apply standardization\n    ","metadata":{}},{"cell_type":"markdown","source":"**Note: the following code requires simplication!**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ncat_features = ['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'Deck', 'Side', 'GroupSize']\nnum_features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n\nclass GroupSizeSplits(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        groupsize = np.array(X[\"GroupSize\"], dtype='int').copy()\n        labels = np.zeros(len(X), dtype='object')\n        for i in range(len(X)):\n            if (groupsize[i]>=2 and groupsize[i]<5):\n                labels[i]=1\n            elif (groupsize[i]>=5 and groupsize[i]<7):\n                labels[i]=2\n            elif (groupsize[i]==7):\n                labels[i]=3\n            elif (groupsize[i]>=8):\n                labels[i]=4\n        X_new = X.copy()\n        X_new[\"GroupSize\"]=labels\n        return X_new\n\nclass AgeSplits(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        age = np.array(X[\"Age\"])\n        labels = np.zeros(len(X), dtype='object')\n        for i in range(len(X)):\n            if age[i] >18 and age[i] <=28:\n                labels[i]=1\n            elif age[i] >28 and age[i] <=38:\n                labels[i]=2\n            elif age[i] >38 and age[i] <=48:\n                labels[i]=3\n            elif age[i] >48 and age[i] <=58:\n                labels[i]=4\n            elif age[i] >58:\n                labels[i]=5\n        X_new = X.copy()\n        X_new[\"Age\"]=labels\n        return X_new\n\nclass ImputerOneHot_df(BaseEstimator, TransformerMixin):\n    #column_names = []\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        imputer = SimpleImputer(strategy=\"most_frequent\")\n        onehot_encoder = OneHotEncoder()\n        X_impute = imputer.fit_transform(X)\n        X_onehot = onehot_encoder.fit_transform(X_impute).toarray()\n        onehot_cats = onehot_encoder.categories_\n        cat_features = X.columns\n        self.column_names = []\n        for i in range(len(cat_features)):\n            name = cat_features[i]\n            cats = onehot_cats[i]\n            for cat in cats:\n                full_name = f'{name}:{cat}'\n                self.column_names.append(full_name) \n        X_new = pd.DataFrame(X_onehot, columns=self.column_names)\n        return X_new    \n\nclass NumTransformer(BaseEstimator, TransformerMixin):\n    #column_names = []\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        imputer = SimpleImputer(strategy=\"median\")\n        std_scaler = StandardScaler()\n        X_impute = imputer.fit_transform(X)\n        X_scaled = std_scaler.fit_transform(X_impute)\n        self.column_names = list(X.columns)\n        X_new = pd.DataFrame(X_scaled, columns=X.columns)\n        return X_new\n    \nclass TotalTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, num_features, cat_features):\n        self.num_features = num_features\n        self.cat_features = cat_features\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        groupsize_encoder = GroupSizeSplits()\n        age_encoder = AgeSplits()        \n        imputeronehot_encoder = ImputerOneHot_df()\n        num_transform = NumTransformer()\n        \n        cat_pipeline = Pipeline([\n            ('groupsize_encoder', groupsize_encoder),\n            ('age_encoder', age_encoder),\n            ('imputeronehot_encoder', imputeronehot_encoder),\n        ])\n        X_cat_transform = cat_pipeline.fit_transform(X[self.cat_features])\n        X_num_transform = num_transform.fit_transform(X[self.num_features])\n        X_new = pd.concat([X_num_transform, X_cat_transform], axis=1)\n        self.column_names = X_new.columns\n        return X_new\n\n\ntransformer = TotalTransformer(num_features, cat_features)\nstrain_prepared = transformer.fit_transform(strain_x)\nval_prepared = transformer.transform(val_x)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:15:15.013917Z","iopub.execute_input":"2022-08-22T14:15:15.014391Z","iopub.status.idle":"2022-08-22T14:15:15.111521Z","shell.execute_reply.started":"2022-08-22T14:15:15.014352Z","shell.execute_reply":"2022-08-22T14:15:15.110591Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Combination / Addition of New Feature(s)**\n\n1. Create `AmountBilled`\n\n2. Create boolean features for `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck`  (Excluded)\n","metadata":{}},{"cell_type":"code","source":"# Create `AmountBilled` and remove `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck` \nlux = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\nstrain_prepared['AmountBilled'] = strain_prepared[lux].sum(axis=1)\n\nstrain_prepared = strain_prepared.drop(lux, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:35:22.824366Z","iopub.execute_input":"2022-08-22T14:35:22.825923Z","iopub.status.idle":"2022-08-22T14:35:22.838509Z","shell.execute_reply.started":"2022-08-22T14:35:22.825879Z","shell.execute_reply":"2022-08-22T14:35:22.837541Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"val_prepared['AmountBilled'] = val_prepared[lux].sum(axis=1)\nval_prepared = val_prepared.drop(lux, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:35:30.479638Z","iopub.execute_input":"2022-08-22T14:35:30.481054Z","iopub.status.idle":"2022-08-22T14:35:30.494138Z","shell.execute_reply.started":"2022-08-22T14:35:30.480994Z","shell.execute_reply":"2022-08-22T14:35:30.492562Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"# **Data Visualization using Different Dimensionality Reduction Techniques**\n\n    - PCA\n    - RBF kernel PCA\n    - Locally linear embedding PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX2D = pca.fit_transform(strain_prepared)\npca.explained_variance_ratio_","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:35:36.956926Z","iopub.execute_input":"2022-08-22T14:35:36.957371Z","iopub.status.idle":"2022-08-22T14:35:37.073366Z","shell.execute_reply.started":"2022-08-22T14:35:36.957337Z","shell.execute_reply":"2022-08-22T14:35:37.070580Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"cols = np.repeat('blue',len(strain_y))\ncols[strain_y == True] = 'red'\nplt.scatter(x=X2D[:,0], y=X2D[:,1], color = cols)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:35:38.506853Z","iopub.execute_input":"2022-08-22T14:35:38.507299Z","iopub.status.idle":"2022-08-22T14:35:38.952814Z","shell.execute_reply.started":"2022-08-22T14:35:38.507266Z","shell.execute_reply":"2022-08-22T14:35:38.951841Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Try kernel PCA\nfrom sklearn.decomposition import KernelPCA\n\nrbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.1)\nX2D_kernel = rbf_pca.fit_transform(strain_prepared)\nplt.scatter(x=X2D_kernel[:,0], y=X2D_kernel[:,1], color = cols)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:35:42.785314Z","iopub.execute_input":"2022-08-22T14:35:42.785964Z","iopub.status.idle":"2022-08-22T14:35:45.964054Z","shell.execute_reply.started":"2022-08-22T14:35:42.785929Z","shell.execute_reply":"2022-08-22T14:35:45.962808Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"# Try Locally Linear Embedding\nfrom sklearn.manifold import LocallyLinearEmbedding\n\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\nX2D_lle = lle.fit_transform(strain_prepared)\n\nplt.scatter(x=X2D_lle[:,0], y=X2D_lle[:,1], color = cols)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:35:46.307907Z","iopub.execute_input":"2022-08-22T14:35:46.308347Z","iopub.status.idle":"2022-08-22T14:35:54.561057Z","shell.execute_reply.started":"2022-08-22T14:35:46.308310Z","shell.execute_reply":"2022-08-22T14:35:54.559804Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"# **Train the model**\n\n1. Logistic Regression\n2. Random Forest\n3. Neural Network","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(max_iter=300)\nlog_reg.fit(strain_prepared, strain_y)\ny_pred = log_reg.predict(val_prepared)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:36:09.549762Z","iopub.execute_input":"2022-08-22T14:36:09.551300Z","iopub.status.idle":"2022-08-22T14:36:09.778673Z","shell.execute_reply.started":"2022-08-22T14:36:09.551243Z","shell.execute_reply":"2022-08-22T14:36:09.777025Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"Accuracy score is used because the distribution of 2 classes are almost equal.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\naccuracy_score(val_y, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:36:13.219588Z","iopub.execute_input":"2022-08-22T14:36:13.220092Z","iopub.status.idle":"2022-08-22T14:36:13.229516Z","shell.execute_reply.started":"2022-08-22T14:36:13.220053Z","shell.execute_reply":"2022-08-22T14:36:13.228286Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# Random Forest\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nparam_grid = [\n    {\n    'max_depth':[4,6,8,10,12]\n    \n    }\n]\nrnd_clf = RandomForestClassifier(bootstrap=True, n_estimators=500, n_jobs=-1,\n                                oob_score=True)\ngrid_search = GridSearchCV(rnd_clf, param_grid, cv=5, scoring='accuracy',\n                          return_train_score=True, refit=True)\ngrid_search.fit(strain_prepared, strain_y)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:36:18.512798Z","iopub.execute_input":"2022-08-22T14:36:18.513266Z","iopub.status.idle":"2022-08-22T14:37:22.912123Z","shell.execute_reply.started":"2022-08-22T14:36:18.513230Z","shell.execute_reply":"2022-08-22T14:37:22.910381Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"grid_search.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:37:22.915068Z","iopub.execute_input":"2022-08-22T14:37:22.915448Z","iopub.status.idle":"2022-08-22T14:37:22.923287Z","shell.execute_reply.started":"2022-08-22T14:37:22.915410Z","shell.execute_reply":"2022-08-22T14:37:22.921548Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"rnd_best = grid_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:37:22.924715Z","iopub.execute_input":"2022-08-22T14:37:22.925166Z","iopub.status.idle":"2022-08-22T14:37:22.936605Z","shell.execute_reply.started":"2022-08-22T14:37:22.925130Z","shell.execute_reply":"2022-08-22T14:37:22.935072Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"rnd_best.oob_score_","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:37:22.939918Z","iopub.execute_input":"2022-08-22T14:37:22.940556Z","iopub.status.idle":"2022-08-22T14:37:22.954934Z","shell.execute_reply.started":"2022-08-22T14:37:22.940503Z","shell.execute_reply":"2022-08-22T14:37:22.952998Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"y_pred = rnd_best.predict(val_prepared)\naccuracy_score(val_y, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:37:22.957002Z","iopub.execute_input":"2022-08-22T14:37:22.957612Z","iopub.status.idle":"2022-08-22T14:37:23.275889Z","shell.execute_reply.started":"2022-08-22T14:37:22.957563Z","shell.execute_reply":"2022-08-22T14:37:23.274481Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# Checking Feature Importance\nfeature_imp = pd.DataFrame()\nfeature_imp['Variable'] = strain_prepared.columns\nfeature_imp[\"Imp\"] = rnd_best.feature_importances_\nfeature_imp.sort_values(by='Imp', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:37:23.278065Z","iopub.execute_input":"2022-08-22T14:37:23.278544Z","iopub.status.idle":"2022-08-22T14:37:23.403120Z","shell.execute_reply.started":"2022-08-22T14:37:23.278496Z","shell.execute_reply":"2022-08-22T14:37:23.401817Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers as layers\n\nmodel = keras.Sequential()\nmodel.add(layers.Dense(60, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(30, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=\"RMSprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(strain_prepared, strain_y, epochs=30, batch_size=100, \n                   validation_data=(val_prepared, val_y))","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:42:30.036402Z","iopub.execute_input":"2022-08-22T14:42:30.037035Z","iopub.status.idle":"2022-08-22T14:42:40.312059Z","shell.execute_reply.started":"2022-08-22T14:42:30.036990Z","shell.execute_reply":"2022-08-22T14:42:40.310712Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"history.history.keys()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:42:43.928318Z","iopub.execute_input":"2022-08-22T14:42:43.929611Z","iopub.status.idle":"2022-08-22T14:42:43.938594Z","shell.execute_reply.started":"2022-08-22T14:42:43.929549Z","shell.execute_reply":"2022-08-22T14:42:43.937317Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"epochs = history.epoch\nhistory = pd.DataFrame(history.history)\naccuracy = history['accuracy']\nval_accuracy = history['val_accuracy']\nloss = history['loss']\nval_loss = history['val_loss']","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:42:44.291700Z","iopub.execute_input":"2022-08-22T14:42:44.292176Z","iopub.status.idle":"2022-08-22T14:42:44.300924Z","shell.execute_reply.started":"2022-08-22T14:42:44.292141Z","shell.execute_reply":"2022-08-22T14:42:44.299857Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"plt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-22T14:42:44.868217Z","iopub.execute_input":"2022-08-22T14:42:44.869291Z","iopub.status.idle":"2022-08-22T14:42:45.481571Z","shell.execute_reply.started":"2022-08-22T14:42:44.869242Z","shell.execute_reply":"2022-08-22T14:42:45.480293Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"There is some problem here that the validation accuracy (loss) is higher (lower) than the training accuracy (loss).","metadata":{}},{"cell_type":"markdown","source":"Solution(s):\n\n- Check stratitiedsplits\n\n- Check how other models performed on the training set and validation set.","metadata":{}},{"cell_type":"markdown","source":"# ","metadata":{}}]}